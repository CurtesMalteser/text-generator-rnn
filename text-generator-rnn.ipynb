{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CurtesMalteser/text-generator-rnn/blob/master/text-generator-rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FqkftZP6c3U",
        "colab_type": "code",
        "outputId": "62be3f23-d5f1-4d3a-ed4c-01a4f0ba9afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Ka1lgx47J8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGZR8bM16wPb",
        "colab_type": "text"
      },
      "source": [
        "### **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzc1g9Fw65CJ",
        "colab_type": "code",
        "outputId": "ee450514-8352-409b-e483-25773029fe49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "with open('drive/My Drive/anna.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "text[:100]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZUTjOFu9vLP",
        "colab_type": "text"
      },
      "source": [
        "### **Tokenization**\n",
        "\n",
        "Here the chars will be converted to and from integers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elzKVJwK97DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we create two dictionaries:\n",
        "# variable names are self explanatory\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zaoe06C7RS1",
        "colab_type": "code",
        "outputId": "43316bf6-5ff0-49f6-e2a2-84216bd2276a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# print encoded chars\n",
        "encoded[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6, 72,  4, 15, 69, 81, 53, 24, 16, 82, 82, 82, 62,  4, 15, 15, 30,\n",
              "       24, 67,  4, 33, 39, 20, 39, 81, 59, 24,  4, 53, 81, 24,  4, 20, 20,\n",
              "       24,  4, 20, 39, 76, 81, 63, 24, 81, 60, 81, 53, 30, 24, 46, 78, 72,\n",
              "        4, 15, 15, 30, 24, 67,  4, 33, 39, 20, 30, 24, 39, 59, 24, 46, 78,\n",
              "       72,  4, 15, 15, 30, 24, 39, 78, 24, 39, 69, 59, 24, 37, 22, 78, 82,\n",
              "       22,  4, 30, 43, 82, 82, 34, 60, 81, 53, 30, 69, 72, 39, 78])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVRqJ32SAKDV",
        "colab_type": "text"
      },
      "source": [
        "### **Pre-Precessing the data**\n",
        "The LSTM expects as input a char converted into int, and the will be converted into one column vector where only the correspending index will have value one and remaing will be 0. This is the **one-hot encoded**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl9KD3XbAR0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlrXnYE6Cbpa",
        "colab_type": "code",
        "outputId": "1483c08f-fa74-43a2-a142-4091675ef91b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# check that one_hot_encoded works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWBjntijDEgJ",
        "colab_type": "text"
      },
      "source": [
        "### **Make mini-batches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kELYCF-UDmv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ej61wbXHcm1",
        "colab_type": "text"
      },
      "source": [
        "### **Test Implementation**\n",
        "\n",
        "\n",
        "*   Batch Size: 8\n",
        "*   Sequence Steps: 50\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69vn3__sDzpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VU7MRzlILFm",
        "colab_type": "code",
        "outputId": "239ff833-f7bf-4cd2-aa4a-c8905dff62ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[ 6 72  4 15 69 81 53 24 16 82]\n",
            " [59 37 78 24 69 72  4 69 24  4]\n",
            " [81 78 57 24 37 53 24  4 24 67]\n",
            " [59 24 69 72 81 24 12 72 39 81]\n",
            " [24 59  4 22 24 72 81 53 24 69]\n",
            " [12 46 59 59 39 37 78 24  4 78]\n",
            " [24  2 78 78  4 24 72  4 57 24]\n",
            " [48 65 20 37 78 59 76 30 43 24]]\n",
            "\n",
            "y\n",
            " [[72  4 15 69 81 53 24 16 82 82]\n",
            " [37 78 24 69 72  4 69 24  4 69]\n",
            " [78 57 24 37 53 24  4 24 67 37]\n",
            " [24 69 72 81 24 12 72 39 81 67]\n",
            " [59  4 22 24 72 81 53 24 69 81]\n",
            " [46 59 59 39 37 78 24  4 78 57]\n",
            " [ 2 78 78  4 24 72  4 57 24 59]\n",
            " [65 20 37 78 59 76 30 43 24 77]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmPY81utiutU",
        "colab_type": "text"
      },
      "source": [
        "### **Define the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3fQ_KN9IfVM",
        "colab_type": "code",
        "outputId": "be6bf7f2-48c1-4b21-cbf6-136732c33fd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Check if the GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if train_on_gpu:\n",
        "    print('Training on GPU!')\n",
        "else:\n",
        "    print('No GPU available! Training on CPU!')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSNH6h1gjzAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextGenRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAcH4UR8s4MY",
        "colab_type": "text"
      },
      "source": [
        "### **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4caO25Rs2eX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: TextGenRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8jKXqnbwiLh",
        "colab_type": "text"
      },
      "source": [
        "### **Intantiating the Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8zCBM_Wtnla",
        "colab_type": "code",
        "outputId": "bfd4f2f0-08a1-4c66-a580-f3ea3b04759a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# Define and print the net\n",
        "n_hidden = 512\n",
        "n_layers = 3\n",
        "\n",
        "net = TextGenRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TextGenRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8aFPrL3xACA",
        "colab_type": "code",
        "outputId": "3dbd953c-0f43-48dd-9a30-ab301ea03ae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 256\n",
        "seq_length = 100\n",
        "n_epochs = 40 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/40... Step: 10... Loss: 3.2107... Val Loss: 3.1169\n",
            "Epoch: 1/40... Step: 20... Loss: 3.1397... Val Loss: 3.1037\n",
            "Epoch: 1/40... Step: 30... Loss: 3.1461... Val Loss: 3.1005\n",
            "Epoch: 1/40... Step: 40... Loss: 3.1376... Val Loss: 3.0970\n",
            "Epoch: 1/40... Step: 50... Loss: 3.1276... Val Loss: 3.0960\n",
            "Epoch: 1/40... Step: 60... Loss: 3.1248... Val Loss: 3.0958\n",
            "Epoch: 2/40... Step: 70... Loss: 3.1344... Val Loss: 3.0945\n",
            "Epoch: 2/40... Step: 80... Loss: 3.0952... Val Loss: 3.0940\n",
            "Epoch: 2/40... Step: 90... Loss: 3.1065... Val Loss: 3.0940\n",
            "Epoch: 2/40... Step: 100... Loss: 3.1153... Val Loss: 3.0925\n",
            "Epoch: 2/40... Step: 110... Loss: 3.1000... Val Loss: 3.0838\n",
            "Epoch: 2/40... Step: 120... Loss: 3.0682... Val Loss: 3.0463\n",
            "Epoch: 2/40... Step: 130... Loss: 3.0203... Val Loss: 2.9890\n",
            "Epoch: 3/40... Step: 140... Loss: 2.9310... Val Loss: 2.8741\n",
            "Epoch: 3/40... Step: 150... Loss: 2.8680... Val Loss: 2.8234\n",
            "Epoch: 3/40... Step: 160... Loss: 2.7806... Val Loss: 2.7323\n",
            "Epoch: 3/40... Step: 170... Loss: 2.7139... Val Loss: 2.6502\n",
            "Epoch: 3/40... Step: 180... Loss: 2.6399... Val Loss: 2.5575\n",
            "Epoch: 3/40... Step: 190... Loss: 2.5694... Val Loss: 2.4787\n",
            "Epoch: 3/40... Step: 200... Loss: 2.4926... Val Loss: 2.4244\n",
            "Epoch: 4/40... Step: 210... Loss: 2.4615... Val Loss: 2.3743\n",
            "Epoch: 4/40... Step: 220... Loss: 2.4132... Val Loss: 2.3352\n",
            "Epoch: 4/40... Step: 230... Loss: 2.3656... Val Loss: 2.3013\n",
            "Epoch: 4/40... Step: 240... Loss: 2.3421... Val Loss: 2.2681\n",
            "Epoch: 4/40... Step: 250... Loss: 2.3240... Val Loss: 2.2378\n",
            "Epoch: 4/40... Step: 260... Loss: 2.2765... Val Loss: 2.2090\n",
            "Epoch: 4/40... Step: 270... Loss: 2.2500... Val Loss: 2.1834\n",
            "Epoch: 5/40... Step: 280... Loss: 2.2190... Val Loss: 2.1579\n",
            "Epoch: 5/40... Step: 290... Loss: 2.2028... Val Loss: 2.1312\n",
            "Epoch: 5/40... Step: 300... Loss: 2.1728... Val Loss: 2.1018\n",
            "Epoch: 5/40... Step: 310... Loss: 2.1413... Val Loss: 2.0731\n",
            "Epoch: 5/40... Step: 320... Loss: 2.1130... Val Loss: 2.0462\n",
            "Epoch: 5/40... Step: 330... Loss: 2.1053... Val Loss: 2.0237\n",
            "Epoch: 5/40... Step: 340... Loss: 2.0972... Val Loss: 2.0001\n",
            "Epoch: 6/40... Step: 350... Loss: 2.0561... Val Loss: 1.9769\n",
            "Epoch: 6/40... Step: 360... Loss: 2.0419... Val Loss: 1.9516\n",
            "Epoch: 6/40... Step: 370... Loss: 2.0355... Val Loss: 1.9271\n",
            "Epoch: 6/40... Step: 380... Loss: 2.0114... Val Loss: 1.9069\n",
            "Epoch: 6/40... Step: 390... Loss: 1.9847... Val Loss: 1.8919\n",
            "Epoch: 6/40... Step: 400... Loss: 1.9722... Val Loss: 1.8682\n",
            "Epoch: 6/40... Step: 410... Loss: 1.9581... Val Loss: 1.8542\n",
            "Epoch: 7/40... Step: 420... Loss: 1.9673... Val Loss: 1.8370\n",
            "Epoch: 7/40... Step: 430... Loss: 1.9133... Val Loss: 1.8162\n",
            "Epoch: 7/40... Step: 440... Loss: 1.8987... Val Loss: 1.7983\n",
            "Epoch: 7/40... Step: 450... Loss: 1.8753... Val Loss: 1.7856\n",
            "Epoch: 7/40... Step: 460... Loss: 1.8641... Val Loss: 1.7663\n",
            "Epoch: 7/40... Step: 470... Loss: 1.8540... Val Loss: 1.7511\n",
            "Epoch: 7/40... Step: 480... Loss: 1.8389... Val Loss: 1.7395\n",
            "Epoch: 8/40... Step: 490... Loss: 1.8180... Val Loss: 1.7261\n",
            "Epoch: 8/40... Step: 500... Loss: 1.7995... Val Loss: 1.7111\n",
            "Epoch: 8/40... Step: 510... Loss: 1.8035... Val Loss: 1.6935\n",
            "Epoch: 8/40... Step: 520... Loss: 1.7807... Val Loss: 1.6815\n",
            "Epoch: 8/40... Step: 530... Loss: 1.7837... Val Loss: 1.6722\n",
            "Epoch: 8/40... Step: 540... Loss: 1.7760... Val Loss: 1.6572\n",
            "Epoch: 8/40... Step: 550... Loss: 1.7380... Val Loss: 1.6449\n",
            "Epoch: 9/40... Step: 560... Loss: 1.7345... Val Loss: 1.6344\n",
            "Epoch: 9/40... Step: 570... Loss: 1.7300... Val Loss: 1.6235\n",
            "Epoch: 9/40... Step: 580... Loss: 1.7258... Val Loss: 1.6103\n",
            "Epoch: 9/40... Step: 590... Loss: 1.7077... Val Loss: 1.6004\n",
            "Epoch: 9/40... Step: 600... Loss: 1.6886... Val Loss: 1.5904\n",
            "Epoch: 9/40... Step: 610... Loss: 1.7059... Val Loss: 1.5808\n",
            "Epoch: 9/40... Step: 620... Loss: 1.6622... Val Loss: 1.5715\n",
            "Epoch: 10/40... Step: 630... Loss: 1.6702... Val Loss: 1.5617\n",
            "Epoch: 10/40... Step: 640... Loss: 1.6638... Val Loss: 1.5544\n",
            "Epoch: 10/40... Step: 650... Loss: 1.6336... Val Loss: 1.5454\n",
            "Epoch: 10/40... Step: 660... Loss: 1.6502... Val Loss: 1.5365\n",
            "Epoch: 10/40... Step: 670... Loss: 1.6468... Val Loss: 1.5266\n",
            "Epoch: 10/40... Step: 680... Loss: 1.6203... Val Loss: 1.5203\n",
            "Epoch: 10/40... Step: 690... Loss: 1.6281... Val Loss: 1.5112\n",
            "Epoch: 11/40... Step: 700... Loss: 1.6108... Val Loss: 1.5049\n",
            "Epoch: 11/40... Step: 710... Loss: 1.5987... Val Loss: 1.4984\n",
            "Epoch: 11/40... Step: 720... Loss: 1.5779... Val Loss: 1.4880\n",
            "Epoch: 11/40... Step: 730... Loss: 1.5809... Val Loss: 1.4841\n",
            "Epoch: 11/40... Step: 740... Loss: 1.5873... Val Loss: 1.4770\n",
            "Epoch: 11/40... Step: 750... Loss: 1.5585... Val Loss: 1.4707\n",
            "Epoch: 12/40... Step: 760... Loss: 1.6133... Val Loss: 1.4661\n",
            "Epoch: 12/40... Step: 770... Loss: 1.5663... Val Loss: 1.4569\n",
            "Epoch: 12/40... Step: 780... Loss: 1.5177... Val Loss: 1.4519\n",
            "Epoch: 12/40... Step: 790... Loss: 1.5543... Val Loss: 1.4458\n",
            "Epoch: 12/40... Step: 800... Loss: 1.5377... Val Loss: 1.4388\n",
            "Epoch: 12/40... Step: 810... Loss: 1.5129... Val Loss: 1.4341\n",
            "Epoch: 12/40... Step: 820... Loss: 1.5277... Val Loss: 1.4290\n",
            "Epoch: 13/40... Step: 830... Loss: 1.5442... Val Loss: 1.4247\n",
            "Epoch: 13/40... Step: 840... Loss: 1.4997... Val Loss: 1.4181\n",
            "Epoch: 13/40... Step: 850... Loss: 1.4837... Val Loss: 1.4143\n",
            "Epoch: 13/40... Step: 860... Loss: 1.4984... Val Loss: 1.4096\n",
            "Epoch: 13/40... Step: 870... Loss: 1.5082... Val Loss: 1.4024\n",
            "Epoch: 13/40... Step: 880... Loss: 1.5033... Val Loss: 1.4006\n",
            "Epoch: 13/40... Step: 890... Loss: 1.4967... Val Loss: 1.3953\n",
            "Epoch: 14/40... Step: 900... Loss: 1.4885... Val Loss: 1.3991\n",
            "Epoch: 14/40... Step: 910... Loss: 1.4627... Val Loss: 1.3900\n",
            "Epoch: 14/40... Step: 920... Loss: 1.4473... Val Loss: 1.3838\n",
            "Epoch: 14/40... Step: 930... Loss: 1.4767... Val Loss: 1.3809\n",
            "Epoch: 14/40... Step: 940... Loss: 1.4817... Val Loss: 1.3748\n",
            "Epoch: 14/40... Step: 950... Loss: 1.4484... Val Loss: 1.3701\n",
            "Epoch: 14/40... Step: 960... Loss: 1.4367... Val Loss: 1.3672\n",
            "Epoch: 15/40... Step: 970... Loss: 1.4348... Val Loss: 1.3630\n",
            "Epoch: 15/40... Step: 980... Loss: 1.4341... Val Loss: 1.3607\n",
            "Epoch: 15/40... Step: 990... Loss: 1.4391... Val Loss: 1.3586\n",
            "Epoch: 15/40... Step: 1000... Loss: 1.4326... Val Loss: 1.3552\n",
            "Epoch: 15/40... Step: 1010... Loss: 1.4158... Val Loss: 1.3490\n",
            "Epoch: 15/40... Step: 1020... Loss: 1.4422... Val Loss: 1.3469\n",
            "Epoch: 15/40... Step: 1030... Loss: 1.4349... Val Loss: 1.3424\n",
            "Epoch: 16/40... Step: 1040... Loss: 1.4128... Val Loss: 1.3444\n",
            "Epoch: 16/40... Step: 1050... Loss: 1.4135... Val Loss: 1.3412\n",
            "Epoch: 16/40... Step: 1060... Loss: 1.4203... Val Loss: 1.3363\n",
            "Epoch: 16/40... Step: 1070... Loss: 1.4175... Val Loss: 1.3331\n",
            "Epoch: 16/40... Step: 1080... Loss: 1.4144... Val Loss: 1.3289\n",
            "Epoch: 16/40... Step: 1090... Loss: 1.4135... Val Loss: 1.3262\n",
            "Epoch: 16/40... Step: 1100... Loss: 1.3969... Val Loss: 1.3219\n",
            "Epoch: 17/40... Step: 1110... Loss: 1.4320... Val Loss: 1.3201\n",
            "Epoch: 17/40... Step: 1120... Loss: 1.3953... Val Loss: 1.3202\n",
            "Epoch: 17/40... Step: 1130... Loss: 1.3943... Val Loss: 1.3175\n",
            "Epoch: 17/40... Step: 1140... Loss: 1.3831... Val Loss: 1.3156\n",
            "Epoch: 17/40... Step: 1150... Loss: 1.3763... Val Loss: 1.3105\n",
            "Epoch: 17/40... Step: 1160... Loss: 1.3852... Val Loss: 1.3083\n",
            "Epoch: 17/40... Step: 1170... Loss: 1.3756... Val Loss: 1.3052\n",
            "Epoch: 18/40... Step: 1180... Loss: 1.3698... Val Loss: 1.3059\n",
            "Epoch: 18/40... Step: 1190... Loss: 1.3522... Val Loss: 1.3031\n",
            "Epoch: 18/40... Step: 1200... Loss: 1.3841... Val Loss: 1.3000\n",
            "Epoch: 18/40... Step: 1210... Loss: 1.3554... Val Loss: 1.2987\n",
            "Epoch: 18/40... Step: 1220... Loss: 1.3728... Val Loss: 1.2954\n",
            "Epoch: 18/40... Step: 1230... Loss: 1.3679... Val Loss: 1.2953\n",
            "Epoch: 18/40... Step: 1240... Loss: 1.3447... Val Loss: 1.2909\n",
            "Epoch: 19/40... Step: 1250... Loss: 1.3453... Val Loss: 1.2924\n",
            "Epoch: 19/40... Step: 1260... Loss: 1.3570... Val Loss: 1.2892\n",
            "Epoch: 19/40... Step: 1270... Loss: 1.3439... Val Loss: 1.2857\n",
            "Epoch: 19/40... Step: 1280... Loss: 1.3526... Val Loss: 1.2823\n",
            "Epoch: 19/40... Step: 1290... Loss: 1.3427... Val Loss: 1.2842\n",
            "Epoch: 19/40... Step: 1300... Loss: 1.3507... Val Loss: 1.2816\n",
            "Epoch: 19/40... Step: 1310... Loss: 1.3210... Val Loss: 1.2780\n",
            "Epoch: 20/40... Step: 1320... Loss: 1.3365... Val Loss: 1.2782\n",
            "Epoch: 20/40... Step: 1330... Loss: 1.3351... Val Loss: 1.2753\n",
            "Epoch: 20/40... Step: 1340... Loss: 1.3327... Val Loss: 1.2728\n",
            "Epoch: 20/40... Step: 1350... Loss: 1.3441... Val Loss: 1.2709\n",
            "Epoch: 20/40... Step: 1360... Loss: 1.3444... Val Loss: 1.2692\n",
            "Epoch: 20/40... Step: 1370... Loss: 1.3216... Val Loss: 1.2696\n",
            "Epoch: 20/40... Step: 1380... Loss: 1.3544... Val Loss: 1.2686\n",
            "Epoch: 21/40... Step: 1390... Loss: 1.3298... Val Loss: 1.2691\n",
            "Epoch: 21/40... Step: 1400... Loss: 1.3127... Val Loss: 1.2682\n",
            "Epoch: 21/40... Step: 1410... Loss: 1.3061... Val Loss: 1.2632\n",
            "Epoch: 21/40... Step: 1420... Loss: 1.3129... Val Loss: 1.2627\n",
            "Epoch: 21/40... Step: 1430... Loss: 1.3218... Val Loss: 1.2598\n",
            "Epoch: 21/40... Step: 1440... Loss: 1.3089... Val Loss: 1.2586\n",
            "Epoch: 22/40... Step: 1450... Loss: 1.3654... Val Loss: 1.2574\n",
            "Epoch: 22/40... Step: 1460... Loss: 1.3162... Val Loss: 1.2572\n",
            "Epoch: 22/40... Step: 1470... Loss: 1.2760... Val Loss: 1.2579\n",
            "Epoch: 22/40... Step: 1480... Loss: 1.3160... Val Loss: 1.2530\n",
            "Epoch: 22/40... Step: 1490... Loss: 1.3086... Val Loss: 1.2534\n",
            "Epoch: 22/40... Step: 1500... Loss: 1.2866... Val Loss: 1.2524\n",
            "Epoch: 22/40... Step: 1510... Loss: 1.3148... Val Loss: 1.2503\n",
            "Epoch: 23/40... Step: 1520... Loss: 1.3151... Val Loss: 1.2526\n",
            "Epoch: 23/40... Step: 1530... Loss: 1.2828... Val Loss: 1.2484\n",
            "Epoch: 23/40... Step: 1540... Loss: 1.2761... Val Loss: 1.2496\n",
            "Epoch: 23/40... Step: 1550... Loss: 1.2936... Val Loss: 1.2446\n",
            "Epoch: 23/40... Step: 1560... Loss: 1.2984... Val Loss: 1.2431\n",
            "Epoch: 23/40... Step: 1570... Loss: 1.3008... Val Loss: 1.2443\n",
            "Epoch: 23/40... Step: 1580... Loss: 1.2948... Val Loss: 1.2430\n",
            "Epoch: 24/40... Step: 1590... Loss: 1.2857... Val Loss: 1.2492\n",
            "Epoch: 24/40... Step: 1600... Loss: 1.2692... Val Loss: 1.2405\n",
            "Epoch: 24/40... Step: 1610... Loss: 1.2623... Val Loss: 1.2414\n",
            "Epoch: 24/40... Step: 1620... Loss: 1.2868... Val Loss: 1.2374\n",
            "Epoch: 24/40... Step: 1630... Loss: 1.2950... Val Loss: 1.2355\n",
            "Epoch: 24/40... Step: 1640... Loss: 1.2785... Val Loss: 1.2374\n",
            "Epoch: 24/40... Step: 1650... Loss: 1.2542... Val Loss: 1.2367\n",
            "Epoch: 25/40... Step: 1660... Loss: 1.2604... Val Loss: 1.2351\n",
            "Epoch: 25/40... Step: 1670... Loss: 1.2682... Val Loss: 1.2318\n",
            "Epoch: 25/40... Step: 1680... Loss: 1.2731... Val Loss: 1.2335\n",
            "Epoch: 25/40... Step: 1690... Loss: 1.2625... Val Loss: 1.2312\n",
            "Epoch: 25/40... Step: 1700... Loss: 1.2543... Val Loss: 1.2276\n",
            "Epoch: 25/40... Step: 1710... Loss: 1.2731... Val Loss: 1.2284\n",
            "Epoch: 25/40... Step: 1720... Loss: 1.2738... Val Loss: 1.2286\n",
            "Epoch: 26/40... Step: 1730... Loss: 1.2566... Val Loss: 1.2286\n",
            "Epoch: 26/40... Step: 1740... Loss: 1.2491... Val Loss: 1.2253\n",
            "Epoch: 26/40... Step: 1750... Loss: 1.2721... Val Loss: 1.2270\n",
            "Epoch: 26/40... Step: 1760... Loss: 1.2580... Val Loss: 1.2265\n",
            "Epoch: 26/40... Step: 1770... Loss: 1.2622... Val Loss: 1.2215\n",
            "Epoch: 26/40... Step: 1780... Loss: 1.2665... Val Loss: 1.2222\n",
            "Epoch: 26/40... Step: 1790... Loss: 1.2487... Val Loss: 1.2229\n",
            "Epoch: 27/40... Step: 1800... Loss: 1.2870... Val Loss: 1.2202\n",
            "Epoch: 27/40... Step: 1810... Loss: 1.2538... Val Loss: 1.2224\n",
            "Epoch: 27/40... Step: 1820... Loss: 1.2497... Val Loss: 1.2197\n",
            "Epoch: 27/40... Step: 1830... Loss: 1.2436... Val Loss: 1.2189\n",
            "Epoch: 27/40... Step: 1840... Loss: 1.2383... Val Loss: 1.2170\n",
            "Epoch: 27/40... Step: 1850... Loss: 1.2594... Val Loss: 1.2167\n",
            "Epoch: 27/40... Step: 1860... Loss: 1.2390... Val Loss: 1.2181\n",
            "Epoch: 28/40... Step: 1870... Loss: 1.2366... Val Loss: 1.2154\n",
            "Epoch: 28/40... Step: 1880... Loss: 1.2270... Val Loss: 1.2191\n",
            "Epoch: 28/40... Step: 1890... Loss: 1.2560... Val Loss: 1.2143\n",
            "Epoch: 28/40... Step: 1900... Loss: 1.2314... Val Loss: 1.2144\n",
            "Epoch: 28/40... Step: 1910... Loss: 1.2425... Val Loss: 1.2134\n",
            "Epoch: 28/40... Step: 1920... Loss: 1.2491... Val Loss: 1.2109\n",
            "Epoch: 28/40... Step: 1930... Loss: 1.2293... Val Loss: 1.2107\n",
            "Epoch: 29/40... Step: 1940... Loss: 1.2285... Val Loss: 1.2130\n",
            "Epoch: 29/40... Step: 1950... Loss: 1.2382... Val Loss: 1.2132\n",
            "Epoch: 29/40... Step: 1960... Loss: 1.2293... Val Loss: 1.2099\n",
            "Epoch: 29/40... Step: 1970... Loss: 1.2338... Val Loss: 1.2097\n",
            "Epoch: 29/40... Step: 1980... Loss: 1.2258... Val Loss: 1.2107\n",
            "Epoch: 29/40... Step: 1990... Loss: 1.2419... Val Loss: 1.2078\n",
            "Epoch: 29/40... Step: 2000... Loss: 1.2100... Val Loss: 1.2085\n",
            "Epoch: 30/40... Step: 2010... Loss: 1.2299... Val Loss: 1.2051\n",
            "Epoch: 30/40... Step: 2020... Loss: 1.2274... Val Loss: 1.2081\n",
            "Epoch: 30/40... Step: 2030... Loss: 1.2333... Val Loss: 1.2050\n",
            "Epoch: 30/40... Step: 2040... Loss: 1.2399... Val Loss: 1.2031\n",
            "Epoch: 30/40... Step: 2050... Loss: 1.2378... Val Loss: 1.2063\n",
            "Epoch: 30/40... Step: 2060... Loss: 1.2196... Val Loss: 1.2031\n",
            "Epoch: 30/40... Step: 2070... Loss: 1.2601... Val Loss: 1.2043\n",
            "Epoch: 31/40... Step: 2080... Loss: 1.2352... Val Loss: 1.2048\n",
            "Epoch: 31/40... Step: 2090... Loss: 1.2190... Val Loss: 1.2042\n",
            "Epoch: 31/40... Step: 2100... Loss: 1.2059... Val Loss: 1.2012\n",
            "Epoch: 31/40... Step: 2110... Loss: 1.2118... Val Loss: 1.1999\n",
            "Epoch: 31/40... Step: 2120... Loss: 1.2170... Val Loss: 1.2025\n",
            "Epoch: 31/40... Step: 2130... Loss: 1.2145... Val Loss: 1.2004\n",
            "Epoch: 32/40... Step: 2140... Loss: 1.2764... Val Loss: 1.2005\n",
            "Epoch: 32/40... Step: 2150... Loss: 1.2172... Val Loss: 1.1990\n",
            "Epoch: 32/40... Step: 2160... Loss: 1.1840... Val Loss: 1.1993\n",
            "Epoch: 32/40... Step: 2170... Loss: 1.2226... Val Loss: 1.1979\n",
            "Epoch: 32/40... Step: 2180... Loss: 1.2122... Val Loss: 1.1968\n",
            "Epoch: 32/40... Step: 2190... Loss: 1.2004... Val Loss: 1.1989\n",
            "Epoch: 32/40... Step: 2200... Loss: 1.2190... Val Loss: 1.1979\n",
            "Epoch: 33/40... Step: 2210... Loss: 1.2239... Val Loss: 1.1952\n",
            "Epoch: 33/40... Step: 2220... Loss: 1.1934... Val Loss: 1.1966\n",
            "Epoch: 33/40... Step: 2230... Loss: 1.1922... Val Loss: 1.1968\n",
            "Epoch: 33/40... Step: 2240... Loss: 1.2039... Val Loss: 1.1944\n",
            "Epoch: 33/40... Step: 2250... Loss: 1.2090... Val Loss: 1.1933\n",
            "Epoch: 33/40... Step: 2260... Loss: 1.2143... Val Loss: 1.1940\n",
            "Epoch: 33/40... Step: 2270... Loss: 1.2087... Val Loss: 1.1940\n",
            "Epoch: 34/40... Step: 2280... Loss: 1.1978... Val Loss: 1.1937\n",
            "Epoch: 34/40... Step: 2290... Loss: 1.1893... Val Loss: 1.1933\n",
            "Epoch: 34/40... Step: 2300... Loss: 1.1790... Val Loss: 1.1917\n",
            "Epoch: 34/40... Step: 2310... Loss: 1.2037... Val Loss: 1.1921\n",
            "Epoch: 34/40... Step: 2320... Loss: 1.2118... Val Loss: 1.1940\n",
            "Epoch: 34/40... Step: 2330... Loss: 1.2018... Val Loss: 1.1913\n",
            "Epoch: 34/40... Step: 2340... Loss: 1.1876... Val Loss: 1.1930\n",
            "Epoch: 35/40... Step: 2350... Loss: 1.1825... Val Loss: 1.1901\n",
            "Epoch: 35/40... Step: 2360... Loss: 1.1984... Val Loss: 1.1907\n",
            "Epoch: 35/40... Step: 2370... Loss: 1.2018... Val Loss: 1.1907\n",
            "Epoch: 35/40... Step: 2380... Loss: 1.1875... Val Loss: 1.1886\n",
            "Epoch: 35/40... Step: 2390... Loss: 1.1819... Val Loss: 1.1908\n",
            "Epoch: 35/40... Step: 2400... Loss: 1.2003... Val Loss: 1.1876\n",
            "Epoch: 35/40... Step: 2410... Loss: 1.1993... Val Loss: 1.1886\n",
            "Epoch: 36/40... Step: 2420... Loss: 1.1848... Val Loss: 1.1891\n",
            "Epoch: 36/40... Step: 2430... Loss: 1.1817... Val Loss: 1.1879\n",
            "Epoch: 36/40... Step: 2440... Loss: 1.1932... Val Loss: 1.1864\n",
            "Epoch: 36/40... Step: 2450... Loss: 1.1882... Val Loss: 1.1870\n",
            "Epoch: 36/40... Step: 2460... Loss: 1.1907... Val Loss: 1.1855\n",
            "Epoch: 36/40... Step: 2470... Loss: 1.1903... Val Loss: 1.1845\n",
            "Epoch: 36/40... Step: 2480... Loss: 1.1805... Val Loss: 1.1866\n",
            "Epoch: 37/40... Step: 2490... Loss: 1.2205... Val Loss: 1.1826\n",
            "Epoch: 37/40... Step: 2500... Loss: 1.1841... Val Loss: 1.1900\n",
            "Epoch: 37/40... Step: 2510... Loss: 1.1830... Val Loss: 1.1845\n",
            "Epoch: 37/40... Step: 2520... Loss: 1.1760... Val Loss: 1.1841\n",
            "Epoch: 37/40... Step: 2530... Loss: 1.1682... Val Loss: 1.1843\n",
            "Epoch: 37/40... Step: 2540... Loss: 1.1952... Val Loss: 1.1818\n",
            "Epoch: 37/40... Step: 2550... Loss: 1.1760... Val Loss: 1.1812\n",
            "Epoch: 38/40... Step: 2560... Loss: 1.1832... Val Loss: 1.1821\n",
            "Epoch: 38/40... Step: 2570... Loss: 1.1668... Val Loss: 1.1861\n",
            "Epoch: 38/40... Step: 2580... Loss: 1.1879... Val Loss: 1.1833\n",
            "Epoch: 38/40... Step: 2590... Loss: 1.1647... Val Loss: 1.1851\n",
            "Epoch: 38/40... Step: 2600... Loss: 1.1793... Val Loss: 1.1811\n",
            "Epoch: 38/40... Step: 2610... Loss: 1.1782... Val Loss: 1.1810\n",
            "Epoch: 38/40... Step: 2620... Loss: 1.1697... Val Loss: 1.1792\n",
            "Epoch: 39/40... Step: 2630... Loss: 1.1751... Val Loss: 1.1809\n",
            "Epoch: 39/40... Step: 2640... Loss: 1.1801... Val Loss: 1.1820\n",
            "Epoch: 39/40... Step: 2650... Loss: 1.1642... Val Loss: 1.1802\n",
            "Epoch: 39/40... Step: 2660... Loss: 1.1708... Val Loss: 1.1808\n",
            "Epoch: 39/40... Step: 2670... Loss: 1.1729... Val Loss: 1.1778\n",
            "Epoch: 39/40... Step: 2680... Loss: 1.1803... Val Loss: 1.1795\n",
            "Epoch: 39/40... Step: 2690... Loss: 1.1548... Val Loss: 1.1776\n",
            "Epoch: 40/40... Step: 2700... Loss: 1.1691... Val Loss: 1.1796\n",
            "Epoch: 40/40... Step: 2710... Loss: 1.1647... Val Loss: 1.1817\n",
            "Epoch: 40/40... Step: 2720... Loss: 1.1724... Val Loss: 1.1775\n",
            "Epoch: 40/40... Step: 2730... Loss: 1.1810... Val Loss: 1.1764\n",
            "Epoch: 40/40... Step: 2740... Loss: 1.1772... Val Loss: 1.1762\n",
            "Epoch: 40/40... Step: 2750... Loss: 1.1557... Val Loss: 1.1808\n",
            "Epoch: 40/40... Step: 2760... Loss: 1.2051... Val Loss: 1.1756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS2q9U1nUu6a",
        "colab_type": "text"
      },
      "source": [
        "### **Checkpoint**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFFRla9OVG4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_40_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-p8F5yoVMXD",
        "colab_type": "text"
      },
      "source": [
        "### **Making Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Um89ZWVn9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE3RmUBnVsMt",
        "colab_type": "text"
      },
      "source": [
        "### **Priming and generating text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrwWxyT6VxJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTx_yABVVzQn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "d3a91936-cec4-445b-dcac-b2612cfacfd3"
      },
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna, as it were, there was no one to\n",
            "see about it at the poor, with his\n",
            "fingers that in his stopped water was standing as she was satidfilled with him, and hoped that she had any sincerity or silent tried. All the conversation was not more than he saw\n",
            "that it was not to blame. But she had telling the friend of\n",
            "how all that would be badly such\n",
            "a single word or take a sight of such sense of such millions as she had been said: The\n",
            "princess was not all the same to the coldness of her husband, who had not succeeded, the sight of several times, he\n",
            "stood as\n",
            "though there was a missake that she was so thinking along in his hand. The midst was passing that he was to be superfly to him, had\n",
            "studying the position of the stray the conversation and\n",
            "world a peasant arouse in his sister out.\n",
            "\n",
            "\"Ah! you know what that's now\n",
            "that I won't be trouble? We should not help been first to take all oud of the success of her. The carriage they were ten town in the\n",
            "conversation. I won't be carting for this; I was the \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2n0sknYV2j5",
        "colab_type": "text"
      },
      "source": [
        "### **Loading a checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1ryRe0-V16B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e1696413-2fe1-426f-fab6-707ec93229e8"
      },
      "source": [
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open('rnn_40_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = TextGenRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihWlihg7V9rF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "outputId": "08e23a56-784c-4430-9a34-88711e70281e"
      },
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime=\"And Antonio said\"))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And Antonio said: \"Where dresses you\n",
            "are,\" said Levin, with a table, with a cold conversation and last past.\n",
            "\n",
            "\"I always come, but I can see you to tell you, we have no strange arrival with the world of such mans of a common waltz,\"\n",
            "said Levin.\n",
            "\n",
            "\"Yes, but there's no day to do it,\" said Stepan Arkadyevitch.\n",
            "\n",
            "\"Oh, where is something to be dread?\"\n",
            "\n",
            "\"Why did you talk about it, and there is such a little wife of the window when I was sincere. He decosped it, but I\n",
            "were a cource of his\n",
            "more and as though he would have a simply astating my subject, and that's all\n",
            "have to be so much.... I've so concerned that they cannot help\n",
            "making him in the same place.. I don't know how it has the same conforming house within my heart; and there was a more standing and considered all all the people, and I have stood as always too much, who came\n",
            "out into such a shoulders in his hands, but she could not be as a peeple. I came over\n",
            "home. Tell her that there was an instant when I can't seed take him to my mind that the certain country all the party, though it's\n",
            "something not assuring him.\"\n",
            "\n",
            "\"Why, there's no our sense, I don't care, but you don't know what I should have said we were at our place, will not give her a mather. And that to be all on the consciousness of all to me, the delightful sense of a corridor.\"\n",
            "\n",
            "\"Well, I'm all dinner to me, but you want to be taken to the way. And what do you\n",
            "see him to make a portraits of the province,\" she said, and without looking significantly\n",
            "from the same son, showed his brother's wife, she seemed to say to\n",
            "him as the princess.\n",
            "\n",
            "He could not be alone, so as to\n",
            "think about himself to be so as such more as though his close and marshal of the sears of the stockings. All the\n",
            "consequence of the millions was a change.\n",
            "\n",
            "\"Well, I\n",
            "shall be driven out, and why not so?\"\n",
            "\n",
            "And he saw since his where she had been the same steps, the cold, she had not asked.\n",
            "\n",
            "\"And the deceive minute we shall be a long and pretty minutes terrible as it\n",
            "will be stupbed in the middle of the clearness. I\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}