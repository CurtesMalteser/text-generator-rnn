{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CurtesMalteser/text-generator-rnn/blob/master/text-generator-rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FqkftZP6c3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5baec79a-e408-42ab-eeb4-2f7130e75416"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Ka1lgx47J8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGZR8bM16wPb",
        "colab_type": "text"
      },
      "source": [
        "### **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzc1g9Fw65CJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "399be7e2-11a3-462c-f772-94ae34481bda"
      },
      "source": [
        "with open('drive/My Drive/anna.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "text[:100]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZUTjOFu9vLP",
        "colab_type": "text"
      },
      "source": [
        "### **Tokenization**\n",
        "\n",
        "Here the chars will be converted to and from integers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elzKVJwK97DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we create two dictionaries:\n",
        "# variable names are self explanatory\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zaoe06C7RS1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "5dd694c8-3a23-41b7-8220-f5498660252d"
      },
      "source": [
        "# print encoded chars\n",
        "encoded[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15, 80, 30, 65, 17, 43,  1, 39, 21, 81, 81, 81, 35, 30, 65, 65, 68,\n",
              "       39, 76, 30,  4,  5, 37,  5, 43, 42, 39, 30,  1, 43, 39, 30, 37, 37,\n",
              "       39, 30, 37,  5, 40, 43, 26, 39, 43, 31, 43,  1, 68, 39,  7, 19, 80,\n",
              "       30, 65, 65, 68, 39, 76, 30,  4,  5, 37, 68, 39,  5, 42, 39,  7, 19,\n",
              "       80, 30, 65, 65, 68, 39,  5, 19, 39,  5, 17, 42, 39, 57, 44, 19, 81,\n",
              "       44, 30, 68, 14, 81, 81, 45, 31, 43,  1, 68, 17, 80,  5, 19])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVRqJ32SAKDV",
        "colab_type": "text"
      },
      "source": [
        "### **Pre-Precessing the data**\n",
        "The LSTM expects as input a char converted into int, and the will be converted into one column vector where only the correspending index will have value one and remaing will be 0. This is the **one-hot encoded**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl9KD3XbAR0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoded(arr, n_labels):\n",
        "\n",
        "  # Initialize the encoded array\n",
        "  one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "\n",
        "  # Fill the appropriate elements with ones\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "\n",
        "  # Finally reshape it to get back to the original array\n",
        "  one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlrXnYE6Cbpa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "cfbf90a4-b1cc-4448-8a2f-d358b8409613"
      },
      "source": [
        "# check that one_hot_encoded works as expected\n",
        "test_seq = np.array([(3, 5, 1)])\n",
        "one_hot = one_hot_encoded(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWBjntijDEgJ",
        "colab_type": "text"
      },
      "source": [
        "### **Make mini-batches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kELYCF-UDmv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "  ''' Create a generator tat returns batches of size: batch_size*seq_length\n",
        "  from arr.\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  arr: Array to generate batches from\n",
        "  batch_size: The number of sequences per batch\n",
        "  seq_length: Number of encoded chars in a sequence\n",
        "  '''\n",
        "\n",
        "  batch_size_total = batch_size * seq_length\n",
        "\n",
        "  # total number of batches we can make\n",
        "  n_batches = len(arr)//batch_size_total\n",
        "\n",
        "  # Keep only enough chars. to make full batches\n",
        "  arr = arr[:n_batches * batch_size_total]\n",
        "\n",
        "  # Reshape into batch_size_rows\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "  # iterate through the array, one sequence at a time\n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "    # The features\n",
        "    x = arr[:, n:n + seq_length]\n",
        "\n",
        "    # The targets, shifted by one\n",
        "    y = np.zeros_like(x)\n",
        "    try:\n",
        "      y[:, :-1], y[:, -1], = x[:, 1:], arr[:, n + seq_length]\n",
        "    except:\n",
        "      y[:, :-1], y[:, -1], = x[:, 1:], arr[:, 0]\n",
        "    yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ej61wbXHcm1",
        "colab_type": "text"
      },
      "source": [
        "### **Test Implementation**\n",
        "\n",
        "\n",
        "*   Batch Size: 8\n",
        "*   Sequence Steps: 50\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69vn3__sDzpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VU7MRzlILFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "e61c5189-e0e5-4441-ba32-84a154806d74"
      },
      "source": [
        "# print first 10 times in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[15 80 30 65 17 43  1 39 21 81]\n",
            " [42 57 19 39 17 80 30 17 39 30]\n",
            " [43 19 82 39 57  1 39 30 39 76]\n",
            " [42 39 17 80 43 39 36 80  5 43]\n",
            " [39 42 30 44 39 80 43  1 39 17]\n",
            " [36  7 42 42  5 57 19 39 30 19]\n",
            " [39 77 19 19 30 39 80 30 82 39]\n",
            " [13 16 37 57 19 42 40 68 14 39]]\n",
            "\n",
            "y\n",
            " [[80 30 65 17 43  1 39 21 81 81]\n",
            " [57 19 39 17 80 30 17 39 30 17]\n",
            " [19 82 39 57  1 39 30 39 76 57]\n",
            " [39 17 80 43 39 36 80  5 43 76]\n",
            " [42 30 44 39 80 43  1 39 17 43]\n",
            " [ 7 42 42  5 57 19 39 30 19 82]\n",
            " [77 19 19 30 39 80 30 82 39 42]\n",
            " [16 37 57 19 42 40 68 14 39 38]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmPY81utiutU",
        "colab_type": "text"
      },
      "source": [
        "### **Define the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3fQ_KN9IfVM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5fa6497d-76de-4a63-dc4f-35a42399f788"
      },
      "source": [
        "# Check if the GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if train_on_gpu:\n",
        "    print('Training on GPU!')\n",
        "else:\n",
        "    print('No GPU available! Training on CPU!')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSNH6h1gjzAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextGenRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=1e-3):\n",
        "        super().__init__()\n",
        "        self.n_hidden = n_hidden\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.drop_prob = drop_prob\n",
        "        self.lr = lr\n",
        "\n",
        "        # Creating chars. dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        # Length of one hot encoded => len(self.chars)\n",
        "        self.lstm = nn.LSTM(len(self.chars),\n",
        "                            hidden_size=self.n_hidden,\n",
        "                            num_layers=n_layers,\n",
        "                            dropout=drop_prob,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(in_features=self.n_hidden,\n",
        "                            out_features=len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        ''''Forward pass through the network.\n",
        "       These inputs are x, and the hidden/cell state \"hidden\".\n",
        "       '''\n",
        "\n",
        "        # Get outputs and the new hidden state from the LSTM\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        # Pass through dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # stack up LSTM outputs using view\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''''Initialize hidden state.'''\n",
        "        # Create twon new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero for hidden stater and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "\n",
        "        return hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAcH4UR8s4MY",
        "colab_type": "text"
      },
      "source": [
        "### **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4caO25Rs2eX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=1e-3, clip=5, val_frac=0.1, print_every=10):\n",
        "  ''' Training a Network\n",
        "\n",
        "      Args\n",
        "      ----\n",
        "      net: TextGenRNN network\n",
        "      data: Text data to train the network\n",
        "      epochs: Number of epochs to train\n",
        "      batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "      seq_length: Number of character steps per mini-batch\n",
        "      lr: learning rate\n",
        "      clip: Gradient clipping\n",
        "      val_frac: Fraction of data to hold for validation\n",
        "  '''\n",
        "  opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "  # Create training and validation data\n",
        "  val_idx = int(len(data)*(1-val_frac))\n",
        "  data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "  if(train_on_gpu):\n",
        "      net.cuda()\n",
        "    \n",
        "  counter = 0\n",
        "  n_chars = len(net.chars)\n",
        "  for e in range(epochs):\n",
        "      # Initialize hidden state\n",
        "      h = net.init_hidden(batch_size)\n",
        "        \n",
        "      for x, y in get_batches(data, batch_size, seq_length):\n",
        "          counter += 1\n",
        "            \n",
        "          # One-hot encode our data and make them Torch tensors\n",
        "          x = one_hot_encoded(x, n_chars)\n",
        "          inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "          if(train_on_gpu):\n",
        "              inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "          # Creating new variables for the hidden state, otherwise\n",
        "          # we'd backprop through the entire training history\n",
        "          h = tuple([each.data for each in h])\n",
        "\n",
        "          # zero accumulated gradients\n",
        "          net.zero_grad()\n",
        "            \n",
        "          # get the output from the model\n",
        "          output, h = net(inputs, h)\n",
        "            \n",
        "          # calculate the loss and perform backprop\n",
        "          loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "          loss.backward()\n",
        "          # \"clip_grad_norm\" helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "          nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "          opt.step()\n",
        "            \n",
        "          # loss stats\n",
        "          if counter % print_every == 0:\n",
        "              # Get validation loss\n",
        "              val_h = net.init_hidden(batch_size)\n",
        "              val_losses = []\n",
        "              net.eval()\n",
        "              for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                  # One-hot encode our data and make them Torch tensors\n",
        "                  x = one_hot_encoded(x, n_chars)\n",
        "                  x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                  # Creating new variables for the hidden state, otherwise\n",
        "                  # we'd backprop through the entire training history\n",
        "                  val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                  inputs, targets = x, y\n",
        "                  if(train_on_gpu):\n",
        "                      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                  output, val_h = net(inputs, val_h)\n",
        "                  val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                  val_losses.append(val_loss.item())\n",
        "                \n",
        "              net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8jKXqnbwiLh",
        "colab_type": "text"
      },
      "source": [
        "### **Intantiating the Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8zCBM_Wtnla",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "11251a60-663f-4d9e-9a0d-aef3f6f7800f"
      },
      "source": [
        "# Define and print the net\n",
        "n_hidden = 512\n",
        "n_layers = 3\n",
        "\n",
        "net = TextGenRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TextGenRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8aFPrL3xACA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "73fb64d1-ce38-4bfb-abd0-6db592f472da"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 50\n",
        "\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50... Step: 10... Loss: 5.2035... Val Loss: 4.9594\n",
            "Epoch: 1/50... Step: 20... Loss: 5.1304... Val Loss: 4.8510\n",
            "Epoch: 1/50... Step: 30... Loss: 5.0925... Val Loss: 4.8122\n",
            "Epoch: 1/50... Step: 40... Loss: 5.0920... Val Loss: 4.8048\n",
            "Epoch: 1/50... Step: 50... Loss: 5.0927... Val Loss: 4.7982\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}