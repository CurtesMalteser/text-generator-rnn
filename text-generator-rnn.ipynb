{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CurtesMalteser/text-generator-rnn/blob/master/text-generator-rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FqkftZP6c3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "47806596-c088-4e03-c7d1-8bff91ac3e0b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Ka1lgx47J8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGZR8bM16wPb",
        "colab_type": "text"
      },
      "source": [
        "### **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzc1g9Fw65CJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0033af34-7698-41db-b615-f687a85a1d74"
      },
      "source": [
        "with open('drive/My Drive/anna.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "text[:100]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZUTjOFu9vLP",
        "colab_type": "text"
      },
      "source": [
        "### **Tokenization**\n",
        "\n",
        "Here the chars will be converted to and from integers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elzKVJwK97DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we create two dictionaries:\n",
        "# variable names are self explanatory\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zaoe06C7RS1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "c5b1c614-21cd-4b65-8fed-67b1f0704ac5"
      },
      "source": [
        "# print encoded chars\n",
        "encoded[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([48, 76, 16, 61, 59, 39, 72, 21, 24, 71, 71, 71, 38, 16, 61, 61, 44,\n",
              "       21, 11, 16, 32, 53, 78, 53, 39, 79, 21, 16, 72, 39, 21, 16, 78, 78,\n",
              "       21, 16, 78, 53, 62, 39, 82, 21, 39, 63, 39, 72, 44, 21, 54, 46, 76,\n",
              "       16, 61, 61, 44, 21, 11, 16, 32, 53, 78, 44, 21, 53, 79, 21, 54, 46,\n",
              "       76, 16, 61, 61, 44, 21, 53, 46, 21, 53, 59, 79, 21, 69, 25, 46, 71,\n",
              "       25, 16, 44, 37, 71, 71, 31, 63, 39, 72, 44, 59, 76, 53, 46])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVRqJ32SAKDV",
        "colab_type": "text"
      },
      "source": [
        "### **Pre-Precessing the data**\n",
        "The LSTM expects as input a char converted into int, and the will be converted into one column vector where only the correspending index will have value one and remaing will be 0. This is the **one-hot encoded**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl9KD3XbAR0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoded(arr, n_labels):\n",
        "\n",
        "  # Initialize the encoded array\n",
        "  one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "\n",
        "  # Fill the appropriate elements with ones\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "\n",
        "  # Finally reshape it to get back to the original array\n",
        "  one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlrXnYE6Cbpa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9462b620-66d3-4090-fd34-a93c5303dd82"
      },
      "source": [
        "# check that one_hot_encoded works as expected\n",
        "test_seq = np.array([(3, 5, 1)])\n",
        "one_hot = one_hot_encoded(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWBjntijDEgJ",
        "colab_type": "text"
      },
      "source": [
        "### **Make mini-batches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kELYCF-UDmv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "  ''' Create a generator tat returns batches of size: batch_size*seq_length\n",
        "  from arr.\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  arr: Array to generate batches from\n",
        "  batch_size: The number of sequences per batch\n",
        "  seq_length: Number of encoded chars in a sequence\n",
        "  '''\n",
        "\n",
        "  batch_size_total = batch_size * seq_length\n",
        "\n",
        "  # total number of batches we can make\n",
        "  n_batches = len(arr)//batch_size_total\n",
        "\n",
        "  # Keep only enough chars. to make full batches\n",
        "  arr = arr[:n_batches * batch_size_total]\n",
        "\n",
        "  # Reshape into batch_size_rows\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "  # iterate through the array, one sequence at a time\n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "    # The features\n",
        "    x = arr[:, n:n + seq_length]\n",
        "\n",
        "    # The targets, shifted by one\n",
        "    y = np.zeros_like(x)\n",
        "    try:\n",
        "      y[:, :-1], y[:, -1], = x[:, 1:], arr[:, n + seq_length]\n",
        "    except:\n",
        "      y[:, :-1], y[:, -1], = x[:, 1:], arr[:, 0]\n",
        "    yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ej61wbXHcm1",
        "colab_type": "text"
      },
      "source": [
        "### **Test Implementation**\n",
        "\n",
        "\n",
        "*   Batch Size: 8\n",
        "*   Sequence Steps: 50\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69vn3__sDzpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VU7MRzlILFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "b033590c-725c-4fc8-94d8-d43b2fbc9631"
      },
      "source": [
        "# print first 10 times in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[48 76 16 61 59 39 72 21 24 71]\n",
            " [79 69 46 21 59 76 16 59 21 16]\n",
            " [39 46 33 21 69 72 21 16 21 11]\n",
            " [79 21 59 76 39 21 14 76 53 39]\n",
            " [21 79 16 25 21 76 39 72 21 59]\n",
            " [14 54 79 79 53 69 46 21 16 46]\n",
            " [21 70 46 46 16 21 76 16 33 21]\n",
            " [64 50 78 69 46 79 62 44 37 21]]\n",
            "\n",
            "y\n",
            " [[76 16 61 59 39 72 21 24 71 71]\n",
            " [69 46 21 59 76 16 59 21 16 59]\n",
            " [46 33 21 69 72 21 16 21 11 69]\n",
            " [21 59 76 39 21 14 76 53 39 11]\n",
            " [79 16 25 21 76 39 72 21 59 39]\n",
            " [54 79 79 53 69 46 21 16 46 33]\n",
            " [70 46 46 16 21 76 16 33 21 79]\n",
            " [50 78 69 46 79 62 44 37 21 36]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmPY81utiutU",
        "colab_type": "text"
      },
      "source": [
        "### **Define the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3fQ_KN9IfVM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d4444ad1-0be0-4dca-ed8a-f58299c0fbb2"
      },
      "source": [
        "# Check if the GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if train_on_gpu:\n",
        "    print('Training on GPU!')\n",
        "else:\n",
        "    print('No GPU available! Training on CPU!')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSNH6h1gjzAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextGenRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=1e-3):\n",
        "        super().__init__()\n",
        "        self.n_hidden = self.n_hidden\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.drop_prob = drop_prob\n",
        "        self.lr = lr\n",
        "\n",
        "        # Creating chars. dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        # Length of one hot encoded => len(self.chars)\n",
        "        self.lstm = nn.LSTM(len(self.chars),\n",
        "                            hidden_size=n_hidden,\n",
        "                            num_layers=n_layers,\n",
        "                            dropout=drop_prob,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(in_features=n_hidden,\n",
        "                            out_features=len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        ''''Forward pass through the network.\n",
        "       These inputs are x, and the hidden/cell state \"hidden\".\n",
        "       '''\n",
        "\n",
        "        # Get outputs and the new hidden state from the LSTM\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        # Pass through dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # stack up LSTM outputs using view\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''''Initialize hidden state.'''\n",
        "        # Create twon new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero for hidden stater and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "\n",
        "        return hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAcH4UR8s4MY",
        "colab_type": "text"
      },
      "source": [
        "### **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4caO25Rs2eX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=1e-3, clip=5, val_frac=0.1, print_every=10):\n",
        "  ''' Training a Network\n",
        "\n",
        "      Args\n",
        "      ----\n",
        "      net: TextGenRNN network\n",
        "      data: Text data to train the network\n",
        "      epochs: Number of epochs to train\n",
        "      batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "      seq_length: Number of character steps per mini-batch\n",
        "      lr: learning rate\n",
        "      clip: Gradient clipping\n",
        "      val_frac: Fraction of data to hold for validation\n",
        "  '''\n",
        "  opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "  # Create training and validation data\n",
        "  val_idx = int(len(data)*(1-val_frac))\n",
        "  data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "  if(train_on_gpu):\n",
        "      net.cuda()\n",
        "    \n",
        "  counter = 0\n",
        "  n_chars = len(net.chars)\n",
        "  for e in range(epochs):\n",
        "      # Initialize hidden state\n",
        "      h = net.init_hidden(batch_size)\n",
        "        \n",
        "      for x, y in get_batches(data, batch_size, seq_length):\n",
        "          counter += 1\n",
        "            \n",
        "          # One-hot encode our data and make them Torch tensors\n",
        "          x = one_hot_encoded(x, n_chars)\n",
        "          inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "          if(train_on_gpu):\n",
        "              inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "          # Creating new variables for the hidden state, otherwise\n",
        "          # we'd backprop through the entire training history\n",
        "          h = tuple([each.data for each in h])\n",
        "\n",
        "          # zero accumulated gradients\n",
        "          net.zero_grad()\n",
        "            \n",
        "          # get the output from the model\n",
        "          output, h = net(inputs, h)\n",
        "            \n",
        "          # calculate the loss and perform backprop\n",
        "          loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "          loss.backward()\n",
        "          # \"clip_grad_norm\" helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "          nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "          opt.step()\n",
        "            \n",
        "          # loss stats\n",
        "          if counter % print_every == 0:\n",
        "              # Get validation loss\n",
        "              val_h = net.init_hidden(batch_size)\n",
        "              val_losses = []\n",
        "              net.eval()\n",
        "              for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                  # One-hot encode our data and make them Torch tensors\n",
        "                  x = one_hot_encoded(x, n_chars)\n",
        "                  x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                  # Creating new variables for the hidden state, otherwise\n",
        "                  # we'd backprop through the entire training history\n",
        "                  val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                  inputs, targets = x, y\n",
        "                  if(train_on_gpu):\n",
        "                      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                  output, val_h = net(inputs, val_h)\n",
        "                  val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                  val_losses.append(val_loss.item())\n",
        "                \n",
        "              net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8jKXqnbwiLh",
        "colab_type": "text"
      },
      "source": [
        "### **Intantiating the Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8zCBM_Wtnla",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "a8532230-7d70-4618-a384-f09e2e44890d"
      },
      "source": [
        "# Define and print the net\n",
        "n_hidden = 512\n",
        "n_layers = 3\n",
        "\n",
        "net = TextGenRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TextGenRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8aFPrL3xACA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 50\n",
        "\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}